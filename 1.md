要在你的网站上集成AI分析功能，与AI模型厂商（如OpenAI、Anthropic、Google Gemini，或国内的百度文心、阿里通义、DeepSeek等）进行通信，核心机制是通过 HTTP API（应用程序接口）。



为了安全和稳定性，通常采用 “前端 -> 你的后端 -> AI厂商” 的架构。



以下是实现这一过程的完整技术路线图：



1\. 核心架构原理



绝不要直接在前端（HTML/JS）代码中调用AI厂商的API，因为这会暴露你的 API Key（密钥），导致额度被盗刷。



正确的通信流程如下：



用户 在你的网站输入数据或上传文件。



前端 将数据发送给你自己的 后端服务器。



后端 接收请求，组合 Prompt（提示词），附上保存在服务器端的 API Key。



后端 向 AI厂商服务器 发起 HTTP 请求。



AI厂商 处理后返回结果（JSON格式）。



后端 将结果处理后返回给 前端 展示。



2\. 具体实现步骤

第一步：获取 API Key



去你选择的 AI 厂商官网注册开发者账号，并创建一个 API Key。



国外主流： OpenAI (GPT-4), Anthropic (Claude 3.5).



国内主流（推荐）： DeepSeek（深度求索）、智谱AI（ChatGLM）、阿里通义千问、百度文心一言。



注：大部分厂商现在都兼容 OpenAI 的 SDK 格式，这大大降低了开发难度。



第二步：后端开发 (Python 示例)



假设使用 Python (FastAPI 或 Flask) 作为后端，且使用兼容 OpenAI 格式的 SDK（以调用 DeepSeek 或 OpenAI 为例）。



安装依赖：



code

Bash

download

content\_copy

expand\_less

pip install openai fastapi uvicorn



后端代码 (main.py)：



code

Python

download

content\_copy

expand\_less

from fastapi import FastAPI, HTTPException

from pydantic import BaseModel

from openai import OpenAI

import os



app = FastAPI()



\# 初始化客户端 (如果是DeepSeek或国内模型，需修改base\_url)

client = OpenAI(

&nbsp;   api\_key="sk-your-api-key-here", # 实际开发中请从环境变量读取 os.getenv("API\_KEY")

&nbsp;   base\_url="https://api.deepseek.com" # 示例：DeepSeek的地址，若是OpenAI则不需要此行

)



\# 定义前端传来的数据格式

class AnalyzeRequest(BaseModel):

&nbsp;   user\_content: str



@app.post("/api/analyze")

async def analyze\_text(request: AnalyzeRequest):

&nbsp;   try:

&nbsp;       # 1. 构造提示词 (System Prompt 是定义AI角色的关键)

&nbsp;       system\_prompt = "你是一个专业的数据分析师。请分析用户提供的内容，并给出3点关键洞察。"

&nbsp;       

&nbsp;       # 2. 向AI厂商发起通信

&nbsp;       response = client.chat.completions.create(

&nbsp;           model="deepseek-chat", # 或 "gemini-3-flash"

&nbsp;           messages=\[

&nbsp;               {"role": "system", "content": system\_prompt},

&nbsp;               {"role": "user", "content": request.user\_content}

&nbsp;           ],

&nbsp;           temperature=0.7

&nbsp;       )

&nbsp;       

&nbsp;       # 3. 获取AI的回复

&nbsp;       ai\_analysis = response.choices\[0].message.content

&nbsp;       

&nbsp;       return {"analysis": ai\_analysis}



&nbsp;   except Exception as e:

&nbsp;       raise HTTPException(status\_code=500, detail=str(e))



\# 启动命令: uvicorn main:app --reload

第三步：前端开发 (JavaScript 示例)



前端只需向你的后端发送请求，不需要知道 AI 厂商的存在。



code

JavaScript

download

content\_copy

expand\_less

async function getAIAnalysis(text) {

&nbsp; const outputDiv = document.getElementById('output');

&nbsp; outputDiv.innerText = "分析中...";



&nbsp; try {

&nbsp;   // 发送请求给你的后端

&nbsp;   const response = await fetch('/api/analyze', {

&nbsp;     method: 'POST',

&nbsp;     headers: {

&nbsp;       'Content-Type': 'application/json'

&nbsp;     },

&nbsp;     body: JSON.stringify({ user\_content: text })

&nbsp;   });



&nbsp;   const data = await response.json();

&nbsp;   

&nbsp;   // 展示结果

&nbsp;   outputDiv.innerText = data.analysis;

&nbsp;   

&nbsp; } catch (error) {

&nbsp;   console.error('Error:', error);

&nbsp;   outputDiv.innerText = "分析失败，请稍后再试。";

&nbsp; }

}

3\. 进阶：如何提升体验（流式传输）



如果 AI 分析需要几秒钟甚至更久，让用户干等体验很差。现在的标准做法是 Streaming (流式传输)，即像 ChatGPT 一样一个字一个字地蹦出来。



通信方式变化：



协议： 使用 SSE (Server-Sent Events) 或 WebSocket。



后端改动： API 调用时开启 stream=True，并使用生成器（yield）返回数据。



Python 后端流式示例：



code

Python

download

content\_copy

expand\_less

from fastapi.responses import StreamingResponse



@app.post("/api/analyze-stream")

def analyze\_stream(request: AnalyzeRequest):

&nbsp;   def event\_stream():

&nbsp;       stream = client.chat.completions.create(

&nbsp;           model="deepseek-chat",

&nbsp;           messages=\[...],

&nbsp;           stream=True  # 开启流式

&nbsp;       )

&nbsp;       for chunk in stream:

&nbsp;           content = chunk.choices\[0].delta.content

&nbsp;           if content:

&nbsp;               yield content # 实时推送片段



&nbsp;   return StreamingResponse(event\_stream(), media\_type="text/event-stream")

4\. 关键注意事项



Prompt Engineering (提示词工程)：

“AI分析功能”好不好用，取决于你在后端预设的 System Prompt。你需要花时间调试这个提示词，例如：“你是一个金融分析师，输出格式必须是Markdown表格...”



网络问题 (如果你在国内)：



如果服务器在国内，调用 OpenAI/Google 的 API 通常会被墙或连接超时。



解决方案：



使用国内模型（DeepSeek、通义千问等），速度快且便宜，合规性好。



将服务器部署在海外（如 AWS, Vercel）。



使用 API 代理中转服务。



上下文管理 (Context)：

HTTP 请求是无状态的。如果你希望 AI 记得用户上一句说了什么（多轮对话），你需要将之前的聊天记录（History）存放在你的数据库里，并在每次请求时把历史记录一起发给 AI 厂商。



成本控制：

AI API 是按 Token（字数）收费的。务必在你的后端做限制，例如限制用户每天只能分析 10 次，或者限制输入文本的长度，防止恶意刷爆你的信用卡。



总结



你现在的任务清单：



选定一个模型厂商，申请 API Key。



搭建一个简单的后端（Node.js 或 Python）。



写一个接口，接收前端数据，封装 System Prompt，转发给 AI 厂商。



将返回的数据传回前端展示。

